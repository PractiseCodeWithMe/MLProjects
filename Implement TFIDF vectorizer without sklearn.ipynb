{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in corpus =  746\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('cleaned_strings', 'rb') as f:\n",
    "    corpus = pickle.load(f)\n",
    "\n",
    "print(\"Number of documents in corpus = \",len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_data(dataset,number_of_idfvalues):\n",
    "    #check the dataset is list or not using isinstance keyword\n",
    "    if isinstance(dataset, list):\n",
    "        #initialize a set to store unique words. Set will not have duplicate\n",
    "        uniquewords = set() \n",
    "        #initialize a dict to store the unique words and index\n",
    "        unique_dict = {}\n",
    "        #length of dataset\n",
    "        total_words_dataset = len(dataset)\n",
    "        #loop through all the document in corpus\n",
    "        for row in dataset:\n",
    "            #loop through all the words in a document\n",
    "            for word in row.split(\" \"):\n",
    "                #add the unique words to the set \n",
    "                uniquewords.add(word)\n",
    "        #conver the set to a list and sort it\n",
    "        uniquewords = list(uniquewords)\n",
    "        uniquewords.sort(reverse = True)\n",
    "        #convert the list to dataframe\n",
    "        uniquewords = pd.DataFrame(uniquewords)\n",
    "        #filter the top 'n' idf features\n",
    "        uniquewords = uniquewords.head(number_of_idfvalues)\n",
    "        #convert the dataframe to list. Now the list will have top idf features \n",
    "        uniquewords = list(uniquewords[0])\n",
    "        #Add the unique words to the dictionary\n",
    "        unique_dict = {value:index for index, value in enumerate(uniquewords)}\n",
    "        #call the function IDF to get idf value for each word\n",
    "        IDF_values = IDF(dataset,uniquewords)\n",
    "    else:\n",
    "        #throw exception when the dataset is not a list\n",
    "        raise ValueError('Give the dataset in list format')\n",
    "    return unique_dict, IDF_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IDF(dataset, uniquewords):\n",
    "    #initialize a dict which will store idf words and values\n",
    "    idf_dict = {}\n",
    "    #length of dataset\n",
    "    N = len(dataset)\n",
    "    #loop through all the uniquewords\n",
    "    for word in uniquewords:\n",
    "        #initialize the count variable\n",
    "        count = 0\n",
    "        #run each unique word against each document in corpus. \n",
    "        for document in dataset:\n",
    "            if word in document.split():\n",
    "                #if word exists in a document then increment the count variable\n",
    "                count += 1\n",
    "        #add the word and its idf value to the dict idf_dict\n",
    "        idf_dict[word] = 1 + math.log((1+N)/(1+count)) \n",
    "    return idf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(dataset, unique_dict, IDF_values): \n",
    "    #sparse matrix which stores tfidf value for each word in each document of the corpus.\n",
    "    sparse_matrix = csr_matrix( (len(dataset), len(unique_dict)), dtype='float64')\n",
    "    #find number of words in document by using Counter and split keywords\n",
    "    for row in range(0,len(dataset)):\n",
    "        number_of_words_in_sentence = Counter(dataset[row].split())\n",
    "        #for each word of a document, check if that exists in unique_dict\n",
    "        #if exists, find ifidf_value using the formulat and add it to the sparse matrix\n",
    "        for word in dataset[row].split():\n",
    "            if word in list(unique_dict.keys()): \n",
    "                #formula to find tfidf values\n",
    "                tfidf_value = (number_of_words_in_sentence[word]/len(dataset[row].split())) * IDF_values[word]\n",
    "                sparse_matrix[row, unique_dict[word]] = tfidf_value\n",
    "    #peform l2 normalization on the sparse matrix\n",
    "    output = normalize(sparse_matrix, norm='l2', axis=1, copy=True, return_norm=False)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'zombiez': 0, 'zombie': 1, 'zillion': 2, 'z': 3, 'yun': 4, 'youtube': 5, 'youthful': 6, 'younger': 7, 'young': 8, 'yet': 9, 'yes': 10, 'yelps': 11, 'years': 12, 'year': 13, 'yeah': 14, 'yawn': 15, 'yardley': 16, 'x': 17, 'wrote': 18, 'wrong': 19, 'written': 20, 'writing': 21, 'writers': 22, 'writer': 23, 'write': 24, 'wrap': 25, 'wow': 26, 'woven': 27, 'wouldnt': 28, 'would': 29, 'worthy': 30, 'worthwhile': 31, 'worthless': 32, 'worth': 33, 'worst': 34, 'worse': 35, 'worry': 36, 'world': 37, 'works': 38, 'working': 39, 'worked': 40, 'work': 41, 'words': 42, 'word': 43, 'wooden': 44, 'woo': 45, 'wont': 46, 'wong': 47, 'wonderfully': 48, 'wonderful': 49}\n",
      "{'zombiez': 6.922918004572872, 'zombie': 6.517452896464707, 'zillion': 6.922918004572872, 'z': 6.922918004572872, 'yun': 6.922918004572872, 'youtube': 6.922918004572872, 'youthful': 6.922918004572872, 'younger': 6.922918004572872, 'young': 6.006627272698717, 'yet': 5.824305715904762, 'yes': 6.229770824012927, 'yelps': 6.922918004572872, 'years': 5.05111582767128, 'year': 6.229770824012927, 'yeah': 6.517452896464707, 'yawn': 6.922918004572872, 'yardley': 6.922918004572872, 'x': 6.922918004572872, 'wrote': 6.922918004572872, 'wrong': 6.229770824012927, 'written': 5.536623643452981, 'writing': 4.9770078555175585, 'writers': 6.922918004572872, 'writer': 6.229770824012927, 'write': 6.517452896464707, 'wrap': 6.922918004572872, 'wow': 6.922918004572872, 'woven': 6.922918004572872, 'wouldnt': 6.922918004572872, 'would': 4.283860674957613, 'worthy': 6.922918004572872, 'worthwhile': 6.922918004572872, 'worthless': 6.922918004572872, 'worth': 4.9770078555175585, 'worst': 5.218169912334447, 'worse': 5.218169912334447, 'worry': 6.922918004572872, 'world': 5.824305715904762, 'works': 5.824305715904762, 'working': 6.517452896464707, 'worked': 6.922918004572872, 'work': 4.671626205966376, 'words': 6.229770824012927, 'word': 5.824305715904762, 'wooden': 6.517452896464707, 'woo': 6.922918004572872, 'wont': 6.922918004572872, 'wong': 6.922918004572872, 'wonderfully': 6.229770824012927, 'wonderful': 4.671626205966376}\n"
     ]
    }
   ],
   "source": [
    "unique_dict, IDF_values = fit_data(corpus, 50)\n",
    "print(unique_dict)\n",
    "print(IDF_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(746, 50)\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\_index.py:82: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n"
     ]
    }
   ],
   "source": [
    "matrix=transform(corpus,unique_dict,IDF_values)\n",
    "print(matrix.shape) \n",
    "print(matrix[0].toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
